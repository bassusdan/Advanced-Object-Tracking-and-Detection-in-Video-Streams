{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6GGEl4V5eTn"
      },
      "source": [
        "# Assignment 2 – Computer Vision\n",
        "## Problem Statement 3: Advanced Object Tracking and Detection in Video Streams\n",
        "**Group 23** | CV_assignment2_group_23_3\n",
        "\n",
        "---\n",
        "\n",
        "### Notebook Structure\n",
        "1. Setup & Installation\n",
        "2. Data Loading & Preprocessing\n",
        "3. Data Augmentation & Visualization\n",
        "4. Model Definition (Faster R-CNN)\n",
        "5. Training\n",
        "6. Tracking Implementation (SORT + Kalman Filter)\n",
        "7. Evaluation (mAP, Tracking Accuracy, ID Switches)\n",
        "8. Model Analysis & Justification\n",
        "9. Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oUC2RPM5pDI"
      },
      "source": [
        "## 1. Setup & Installation\n",
        "Install all required libraries silently. We use PyTorch for the Faster R-CNN model,\n",
        "albumentations for augmentation, and FilterPy for the Kalman Filter used in SORT tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSxP_FUu4sZ4"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Suppress installation output to keep the notebook clean.\n",
        "# We install:\n",
        "#   - torch/torchvision: deep learning framework and pre-trained Faster R-CNN\n",
        "#   - albumentations: powerful image augmentation library\n",
        "#   - filterpy: provides Kalman Filter implementation for SORT tracker\n",
        "#   - motmetrics: standard MOT evaluation metrics (MOTA, IDF1, ID switches)\n",
        "!pip install torch torchvision albumentations filterpy motmetrics opencv-python-headless"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PT9vq9cr5z63",
        "outputId": "d8e462fe-9d3f-45f5-9efc-d040553790ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import warnings\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from filterpy.kalman import KalmanFilter\n",
        "import motmetrics as mm\n",
        "\n",
        "# ── Reproducibility ──────────────────────────────────────────\n",
        "# Setting seeds ensures results are reproducible across runs.\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Suppress non-critical warnings for cleaner output.\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Use GPU if available (Colab T4), otherwise fall back to CPU.\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_Ip9fRd54ir"
      },
      "source": [
        "## 2. Data Loading & Preprocessing\n",
        "We download the **MOT17** dataset (a standard benchmark from MOTChallenge) and extract\n",
        "frames and ground-truth annotations. We use the `MOT17-02`, `MOT17-09`, and `MOT17-05`\n",
        "sequences which provide diverse scenarios (static/moving camera, varying crowd density)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ-OV_u957y4",
        "outputId": "42264b53-a7a5-4b34-9439-dce8c3d9d1a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading MOT17 dataset (this may take a few minutes)...\n"
          ]
        }
      ],
      "source": [
        "# ── Download MOT17 subset ─────────────────────────────────────\n",
        "# MOT17 is one of the most widely used multi-object tracking benchmarks.\n",
        "# We download just the training split (which has ground-truth annotations)\n",
        "# so we can both train and evaluate our model.\n",
        "\n",
        "DATA_DIR = Path(\"mot17_data\")\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "MOT17_URL = \"https://motchallenge.net/data/MOT17.zip\"\n",
        "ZIP_PATH = DATA_DIR / \"MOT17.zip\"\n",
        "\n",
        "if not (DATA_DIR / \"MOT17\").exists():\n",
        "    print(\"Downloading MOT17 dataset (this may take a few minutes)...\")\n",
        "    urllib.request.urlretrieve(MOT17_URL, ZIP_PATH)\n",
        "    print(\"Extracting...\")\n",
        "    with zipfile.ZipFile(ZIP_PATH, 'r') as zf:\n",
        "        zf.extractall(DATA_DIR)\n",
        "    ZIP_PATH.unlink()  # Remove zip to save disk space\n",
        "    print(\"Done!\")\n",
        "else:\n",
        "    print(\"MOT17 dataset already present.\")\n",
        "\n",
        "# We focus on sequences that use the DPM detector annotations\n",
        "# to keep training manageable on a single GPU.\n",
        "SEQUENCES = [\"MOT17-02-DPM\", \"MOT17-09-DPM\", \"MOT17-05-DPM\"]\n",
        "TRAIN_ROOT = DATA_DIR / \"MOT17\" / \"train\"\n",
        "print(f\"Available sequences: {[s for s in os.listdir(TRAIN_ROOT) if os.path.isdir(TRAIN_ROOT / s)]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iLg3Xwc76i4"
      },
      "outputs": [],
      "source": [
        "# ── Parse MOT ground-truth annotations ────────────────────────\n",
        "# The MOT format stores annotations in a CSV file (gt.txt) with columns:\n",
        "#   frame, id, bb_left, bb_top, bb_width, bb_height, conf, class, visibility\n",
        "# We parse these into a dict: {frame_number: [(id, x1, y1, x2, y2), ...]}\n",
        "\n",
        "def parse_mot_annotations(gt_path):\n",
        "    \"\"\"Parse MOTChallenge ground-truth file into per-frame annotations.\n",
        "\n",
        "    We filter to keep only pedestrian annotations (class 1) with\n",
        "    visibility > 0.25, which removes heavily occluded or ambiguous objects.\n",
        "    \"\"\"\n",
        "    annotations = defaultdict(list)\n",
        "    with open(gt_path, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(',')\n",
        "            frame_id = int(parts[0])\n",
        "            track_id = int(parts[1])\n",
        "            x, y, w, h = float(parts[2]), float(parts[3]), float(parts[4]), float(parts[5])\n",
        "            conf = float(parts[6])\n",
        "            cls = int(parts[7])\n",
        "            visibility = float(parts[8])\n",
        "            # Keep only confident pedestrian annotations that are reasonably visible\n",
        "            if conf == 0 or cls != 1 or visibility < 0.25:\n",
        "                continue\n",
        "            # Convert from (x, y, w, h) to (x1, y1, x2, y2) format\n",
        "            annotations[frame_id].append({\n",
        "                'track_id': track_id,\n",
        "                'bbox': [x, y, x + w, y + h]\n",
        "            })\n",
        "    return dict(annotations)\n",
        "\n",
        "# Build a combined dataset of (image_path, annotations) pairs\n",
        "all_samples = []\n",
        "for seq in SEQUENCES:\n",
        "    seq_dir = TRAIN_ROOT / seq\n",
        "    gt_path = seq_dir / \"gt\" / \"gt.txt\"\n",
        "    img_dir = seq_dir / \"img1\"\n",
        "    annotations = parse_mot_annotations(gt_path)\n",
        "    for frame_id, annots in sorted(annotations.items()):\n",
        "        img_path = img_dir / f\"{frame_id:06d}.jpg\"\n",
        "        if img_path.exists():\n",
        "            all_samples.append((str(img_path), annots))\n",
        "\n",
        "print(f\"Total annotated frames: {len(all_samples)}\")\n",
        "print(f\"Example annotation: {all_samples[0][1][:2]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zd_fFPUP7-YB"
      },
      "outputs": [],
      "source": [
        "# ── Normalization & Dataset class ──────────────────────────────\n",
        "# We normalize pixel values to [0, 1] range (standard for PyTorch vision models)\n",
        "# and wrap everything in a proper Dataset class for the DataLoader.\n",
        "\n",
        "class MOTDetectionDataset(data.Dataset):\n",
        "    \"\"\"PyTorch Dataset for MOT object detection.\n",
        "\n",
        "    Each sample returns a normalized image tensor and a target dict\n",
        "    containing bounding boxes, labels, and area — the format expected\n",
        "    by torchvision's Faster R-CNN implementation.\n",
        "    \"\"\"\n",
        "    def __init__(self, samples, transforms=None):\n",
        "        self.samples = samples\n",
        "        self.transforms = transforms  # albumentations pipeline\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, annots = self.samples[idx]\n",
        "\n",
        "        # Read image and convert BGR->RGB (OpenCV loads as BGR)\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        bboxes = [a['bbox'] for a in annots]\n",
        "        track_ids = [a['track_id'] for a in annots]\n",
        "\n",
        "        # Apply augmentations if provided\n",
        "        if self.transforms:\n",
        "            transformed = self.transforms(\n",
        "                image=image,\n",
        "                bboxes=bboxes,\n",
        "                labels=track_ids\n",
        "            )\n",
        "            image = transformed['image']  # already a tensor after ToTensorV2\n",
        "            bboxes = transformed['bboxes']\n",
        "            track_ids = transformed['labels']\n",
        "        else:\n",
        "            # Default: just normalize to [0,1] and convert to tensor\n",
        "            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "        # Build target dict in the format Faster R-CNN expects\n",
        "        boxes = torch.as_tensor(bboxes, dtype=torch.float32).reshape(-1, 4) if bboxes else torch.zeros((0, 4), dtype=torch.float32)\n",
        "        labels = torch.ones(len(bboxes), dtype=torch.int64)  # class 1 = pedestrian\n",
        "        area = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]) if len(bboxes) > 0 else torch.tensor([])\n",
        "\n",
        "        target = {\n",
        "            'boxes': boxes,\n",
        "            'labels': labels,\n",
        "            'image_id': torch.tensor([idx]),\n",
        "            'area': area,\n",
        "            'iscrowd': torch.zeros(len(bboxes), dtype=torch.int64)\n",
        "        }\n",
        "        return image, target\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate: images have different sizes, so we can't stack them.\"\"\"\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "print(\"Dataset class defined successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_UrmW6U8Clk"
      },
      "source": [
        "## 3. Data Augmentation & Visualization\n",
        "We apply three augmentation techniques as required:\n",
        "- **Random Cropping**: Randomly crops a portion of the image, helping the model generalize to partial views.\n",
        "- **Horizontal Flipping**: Mirrors the image, doubling the effective dataset size.\n",
        "- **Color Jittering**: Randomly adjusts brightness, contrast, saturation, and hue to handle varying lighting conditions.\n",
        "\n",
        "We use `albumentations` because it natively supports bounding box transformations,\n",
        "ensuring annotations stay consistent with the augmented image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34RXenDx8FQ5"
      },
      "outputs": [],
      "source": [
        "train_transforms = A.Compose([\n",
        "    # Random Cropping: crop to 80-100% of original, then resize back.\n",
        "    # This simulates zoom-in effects and partial occlusion.\n",
        "    A.RandomResizedCrop(\n",
        "        size=(600, 800), scale=(0.8, 1.0),\n",
        "        ratio=(0.75, 1.33), p=0.5\n",
        "    ),\n",
        "    # Horizontal Flip: 50% chance to mirror the image.\n",
        "    # Pedestrians look the same from either direction.\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    # Color Jitter: randomly alter brightness, contrast, saturation, hue.\n",
        "    # This makes the model robust to different lighting/weather conditions.\n",
        "    A.ColorJitter(\n",
        "        brightness=0.3, contrast=0.3,\n",
        "        saturation=0.3, hue=0.1, p=0.5\n",
        "    ),\n",
        "    # Normalize to [0,1] and convert to PyTorch tensor\n",
        "    A.Normalize(mean=[0, 0, 0], std=[1, 1, 1]),  # simple /255 normalization\n",
        "    ToTensorV2()\n",
        "], bbox_params=A.BboxParams(\n",
        "    format='pascal_voc',\n",
        "    label_fields=['labels'],\n",
        "    min_visibility=0.5, # Increased from 0.3 to 0.5 to be more conservative\n",
        "    clip=True # Explicitly set to True to ensure clipping\n",
        "))\n",
        "\n",
        "# Validation: no augmentation, only normalize\n",
        "val_transforms = A.Compose([\n",
        "    A.Normalize(mean=[0, 0, 0], std=[1, 1, 1]),\n",
        "    ToTensorV2()\n",
        "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'], clip=True)) # Also ensure clipping for validation\n",
        "\n",
        "print(\"Augmentation pipelines ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjur91F68KKb"
      },
      "outputs": [],
      "source": [
        "# ── Visualize augmented samples ────────────────────────────────\n",
        "# Show original vs. augmented images side-by-side to verify that\n",
        "# augmentations are correctly applied and bounding boxes stay aligned.\n",
        "\n",
        "def visualize_sample(img_path, annots, transforms, title=\"Sample\"):\n",
        "    \"\"\"Display an image with its bounding boxes before and after augmentation.\"\"\"\n",
        "    image = cv2.imread(img_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    bboxes = [a['bbox'] for a in annots]\n",
        "    labels = [a['track_id'] for a in annots]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Original image\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].set_title(f\"{title} — Original\")\n",
        "    for bbox in bboxes:\n",
        "        rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2]-bbox[0], bbox[3]-bbox[1],\n",
        "                                  linewidth=2, edgecolor='lime', facecolor='none')\n",
        "        axes[0].add_patch(rect)\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Augmented image\n",
        "    transformed = transforms(image=image, bboxes=bboxes, labels=labels)\n",
        "    aug_img = transformed['image'].permute(1, 2, 0).numpy()\n",
        "    aug_bboxes = transformed['bboxes']\n",
        "\n",
        "    axes[1].imshow(aug_img)\n",
        "    axes[1].set_title(f\"{title} — Augmented\")\n",
        "    for bbox in aug_bboxes:\n",
        "        rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2]-bbox[0], bbox[3]-bbox[1],\n",
        "                                  linewidth=2, edgecolor='cyan', facecolor='none')\n",
        "        axes[1].add_patch(rect)\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Show 3 sample frames with their augmentations\n",
        "for i in range(0, min(6, len(all_samples)), 2):\n",
        "    img_path, annots = all_samples[i]\n",
        "    visualize_sample(img_path, annots, train_transforms, title=f\"Frame {i}\")\n",
        "\n",
        "print(\"Augmentation visualizations complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlWsj74O8QU6"
      },
      "source": [
        "## 4. Model Definition — Faster R-CNN\n",
        "We use a **Faster R-CNN with ResNet-50 FPN v2** backbone pre-trained on COCO.\n",
        "We replace the classification head to predict only 2 classes (background + pedestrian).\n",
        "\n",
        "Faster R-CNN is ideal here because:\n",
        "- It provides both **bounding box regression** and **classification** in a single forward pass.\n",
        "- The Region Proposal Network (RPN) efficiently proposes candidate regions.\n",
        "- Pre-training on COCO gives us strong feature representations for transfer learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w23BaeFj8Skn"
      },
      "outputs": [],
      "source": [
        "# ── Build the Faster R-CNN model ──────────────────────────────\n",
        "# We start from a COCO-pretrained model and replace only the\n",
        "# classification head. This is standard transfer learning: the\n",
        "# backbone features transfer well, but the class-specific head\n",
        "# must be retrained for our pedestrian-only task.\n",
        "\n",
        "def build_model(num_classes=2):\n",
        "    \"\"\"Create a Faster R-CNN model fine-tuned for pedestrian detection.\n",
        "\n",
        "    Args:\n",
        "        num_classes: 2 = background + pedestrian\n",
        "\n",
        "    Returns:\n",
        "        model ready for training on our MOT dataset\n",
        "    \"\"\"\n",
        "    # Load pre-trained Faster R-CNN v2 (improved training recipe)\n",
        "    weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
        "    model = fasterrcnn_resnet50_fpn_v2(weights=weights)\n",
        "\n",
        "    # Replace the pre-trained classification head with a new one\n",
        "    # The input features dimension comes from the existing box predictor\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "model = build_model(num_classes=2)\n",
        "model.to(DEVICE)\n",
        "print(f\"Model loaded on {DEVICE}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3K9tYOt8XcQ"
      },
      "outputs": [],
      "source": [
        "# ── Prepare DataLoaders ───────────────────────────────────────\n",
        "# Split data: 80% train, 20% validation.\n",
        "# We keep sequences together conceptually but shuffle frames for training\n",
        "# to prevent the model from memorizing temporal order.\n",
        "\n",
        "random.shuffle(all_samples)\n",
        "split = int(0.8 * len(all_samples))\n",
        "train_samples = all_samples[:split]\n",
        "val_samples = all_samples[split:]\n",
        "\n",
        "train_dataset = MOTDetectionDataset(train_samples, transforms=train_transforms)\n",
        "val_dataset = MOTDetectionDataset(val_samples, transforms=val_transforms)\n",
        "\n",
        "train_loader = data.DataLoader(\n",
        "    train_dataset, batch_size=4, shuffle=True,\n",
        "    collate_fn=collate_fn, num_workers=2, pin_memory=True\n",
        ")\n",
        "val_loader = data.DataLoader(\n",
        "    val_dataset, batch_size=4, shuffle=False,\n",
        "    collate_fn=collate_fn, num_workers=2, pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(train_dataset)} frames | Val: {len(val_dataset)} frames\")\n",
        "print(f\"Train batches: {len(train_loader)} | Val batches: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zId0Rxzl8aoN"
      },
      "source": [
        "## 5. Training\n",
        "We fine-tune the model for a few epochs with SGD optimizer and a learning rate scheduler.\n",
        "The model is already pre-trained on COCO (which includes person detection), so only a\n",
        "few epochs of fine-tuning are needed to adapt it to the MOT17 domain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPglCOOc8bZM"
      },
      "outputs": [],
      "source": [
        "# ── Training configuration ────────────────────────────────────\n",
        "# SGD with momentum is the standard optimizer for Faster R-CNN.\n",
        "# We use a StepLR scheduler to reduce LR after epoch 3, preventing\n",
        "# overshooting once the model is close to convergence.\n",
        "\n",
        "NUM_EPOCHS = 5\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(), lr=0.005,\n",
        "    momentum=0.9, weight_decay=0.0005\n",
        ")\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "print(f\"Training for {NUM_EPOCHS} epochs\")\n",
        "print(f\"Initial LR: {optimizer.param_groups[0]['lr']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaUeobxz8gPQ"
      },
      "outputs": [],
      "source": [
        "# ── Training loop ─────────────────────────────────────────────\n",
        "# Faster R-CNN in training mode returns a dict of losses:\n",
        "#   - loss_classifier: classification loss for proposals\n",
        "#   - loss_box_reg: bounding box regression loss\n",
        "#   - loss_objectness: RPN objectness loss\n",
        "#   - loss_rpn_box_reg: RPN box regression loss\n",
        "\n",
        "train_losses = []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    for images, targets in train_loader:\n",
        "        # Move data to GPU\n",
        "        images = [img.to(DEVICE) for img in images]\n",
        "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Skip batches where all targets have no boxes (edge case)\n",
        "        if all(t['boxes'].shape[0] == 0 for t in targets):\n",
        "            continue\n",
        "\n",
        "        # Forward pass: model returns loss dict in training mode\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # Backward pass and weight update\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        # Gradient clipping prevents exploding gradients in early epochs\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += losses.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    lr_scheduler.step()\n",
        "    avg_loss = epoch_loss / max(num_batches, 1)\n",
        "    train_losses.append(avg_loss)\n",
        "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] — Loss: {avg_loss:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9nJlaDBAXW_"
      },
      "outputs": [],
      "source": [
        "# ── Plot training loss curve ──────────────────────────────────\n",
        "# A decreasing loss curve confirms the model is learning.\n",
        "# We watch for signs of overfitting (loss divergence) or\n",
        "# underfitting (loss plateauing at a high value).\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(range(1, NUM_EPOCHS + 1), train_losses, 'b-o', linewidth=2, markersize=8)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Training Loss', fontsize=12)\n",
        "plt.title('Faster R-CNN Fine-Tuning Loss', fontsize=14)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqNfvMmiAb9r"
      },
      "source": [
        "## 6. Tracking Implementation — SORT with Kalman Filter\n",
        "We implement the **SORT (Simple Online and Realtime Tracking)** algorithm, which combines:\n",
        "1. **Kalman Filter**: Predicts object positions in the next frame based on constant-velocity motion model.\n",
        "2. **Hungarian Algorithm**: Optimally matches predicted positions to new detections using IoU-based cost matrix.\n",
        "3. **Track Management**: Creates new tracks for unmatched detections and removes tracks that haven't been matched for several frames.\n",
        "\n",
        "This provides **temporal consistency** — objects maintain the same ID across frames,\n",
        "even through brief occlusions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1sF4-UBAZPq"
      },
      "outputs": [],
      "source": [
        "# ── Kalman Filter–based Track ─────────────────────────────────\n",
        "# Each tracked object has its own Kalman Filter that maintains a\n",
        "# state vector [x_center, y_center, area, aspect_ratio, vx, vy, va, vr].\n",
        "# This constant-velocity model is simple but effective for pedestrian tracking.\n",
        "\n",
        "def iou_batch(bb_test, bb_gt):\n",
        "    \"\"\"Compute IoU between two sets of bounding boxes.\n",
        "\n",
        "    This vectorized implementation efficiently computes the full\n",
        "    NxM IoU matrix needed for the Hungarian matching step.\n",
        "    \"\"\"\n",
        "    bb_gt = np.expand_dims(bb_gt, 0)\n",
        "    bb_test = np.expand_dims(bb_test, 1)\n",
        "\n",
        "    xx1 = np.maximum(bb_test[..., 0], bb_gt[..., 0])\n",
        "    yy1 = np.maximum(bb_test[..., 1], bb_gt[..., 1])\n",
        "    xx2 = np.minimum(bb_test[..., 2], bb_gt[..., 2])\n",
        "    yy2 = np.minimum(bb_test[..., 3], bb_gt[..., 3])\n",
        "\n",
        "    w = np.maximum(0., xx2 - xx1)\n",
        "    h = np.maximum(0., yy2 - yy1)\n",
        "    intersection = w * h\n",
        "\n",
        "    area_test = (bb_test[..., 2] - bb_test[..., 0]) * (bb_test[..., 3] - bb_test[..., 1])\n",
        "    area_gt = (bb_gt[..., 2] - bb_gt[..., 0]) * (bb_gt[..., 3] - bb_gt[..., 1])\n",
        "\n",
        "    return intersection / (area_test + area_gt - intersection + 1e-6)\n",
        "\n",
        "\n",
        "def convert_bbox_to_z(bbox):\n",
        "    \"\"\"Convert [x1,y1,x2,y2] to Kalman state [cx, cy, area, ratio].\"\"\"\n",
        "    w = bbox[2] - bbox[0]\n",
        "    h = bbox[3] - bbox[1]\n",
        "    x = bbox[0] + w / 2.\n",
        "    y = bbox[1] + h / 2.\n",
        "    s = w * h\n",
        "    r = w / float(h + 1e-6)\n",
        "    return np.array([x, y, s, r]).reshape((4, 1))\n",
        "\n",
        "\n",
        "def convert_x_to_bbox(x):\n",
        "    \"\"\"Convert Kalman state [cx, cy, area, ratio] back to [x1,y1,x2,y2].\"\"\"\n",
        "    w = np.sqrt(x[2] * x[3])\n",
        "    h = x[2] / (w + 1e-6)\n",
        "    return np.array([\n",
        "        x[0] - w / 2., x[1] - h / 2.,\n",
        "        x[0] + w / 2., x[1] + h / 2.\n",
        "    ]).flatten()\n",
        "\n",
        "\n",
        "class KalmanBoxTracker:\n",
        "    \"\"\"A single tracked object using a Kalman Filter.\n",
        "\n",
        "    The filter estimates [cx, cy, area, ratio, vx, vy, va, vr]\n",
        "    — position and velocity of the bounding box center, area, and aspect ratio.\n",
        "    \"\"\"\n",
        "    count = 0  # Global ID counter for unique track IDs\n",
        "\n",
        "    def __init__(self, bbox):\n",
        "        # Initialize Kalman Filter with 7 state dims and 4 measurement dims\n",
        "        self.kf = KalmanFilter(dim_x=7, dim_z=4)\n",
        "        # State transition matrix: constant velocity model\n",
        "        self.kf.F = np.array([\n",
        "            [1,0,0,0,1,0,0],\n",
        "            [0,1,0,0,0,1,0],\n",
        "            [0,0,1,0,0,0,1],\n",
        "            [0,0,0,1,0,0,0],\n",
        "            [0,0,0,0,1,0,0],\n",
        "            [0,0,0,0,0,1,0],\n",
        "            [0,0,0,0,0,0,1]\n",
        "        ])\n",
        "        # Measurement matrix: we observe [cx, cy, area, ratio]\n",
        "        self.kf.H = np.array([\n",
        "            [1,0,0,0,0,0,0],\n",
        "            [0,1,0,0,0,0,0],\n",
        "            [0,0,1,0,0,0,0],\n",
        "            [0,0,0,1,0,0,0]\n",
        "        ])\n",
        "        # Measurement and process noise covariances (tuned empirically)\n",
        "        self.kf.R[2:, 2:] *= 10.\n",
        "        self.kf.P[4:, 4:] *= 1000.\n",
        "        self.kf.P *= 10.\n",
        "        self.kf.Q[-1, -1] *= 0.01\n",
        "        self.kf.Q[4:, 4:] *= 0.01\n",
        "\n",
        "        self.kf.x[:4] = convert_bbox_to_z(bbox)\n",
        "        self.time_since_update = 0\n",
        "        self.id = KalmanBoxTracker.count\n",
        "        KalmanBoxTracker.count += 1\n",
        "        self.hits = 0\n",
        "        self.hit_streak = 0\n",
        "        self.age = 0\n",
        "\n",
        "    def update(self, bbox):\n",
        "        \"\"\"Update state with a matched detection.\"\"\"\n",
        "        self.time_since_update = 0\n",
        "        self.hits += 1\n",
        "        self.hit_streak += 1\n",
        "        self.kf.update(convert_bbox_to_z(bbox))\n",
        "\n",
        "    def predict(self):\n",
        "        \"\"\"Advance state by one time step and return predicted bbox.\"\"\"\n",
        "        if (self.kf.x[6] + self.kf.x[2]) <= 0:\n",
        "            self.kf.x[6] *= 0.0\n",
        "        self.kf.predict()\n",
        "        self.age += 1\n",
        "        if self.time_since_update > 0:\n",
        "            self.hit_streak = 0\n",
        "        self.time_since_update += 1\n",
        "        return convert_x_to_bbox(self.kf.x)\n",
        "\n",
        "    def get_state(self):\n",
        "        return convert_x_to_bbox(self.kf.x)\n",
        "\n",
        "print(\"Kalman tracker defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YChBLR0yAhTR"
      },
      "outputs": [],
      "source": [
        "# ── SORT Tracker ──────────────────────────────────────────────\n",
        "# SORT ties together Kalman prediction and Hungarian matching.\n",
        "# It maintains a list of active tracks and handles:\n",
        "#   - Matching detections to existing tracks (via IoU + Hungarian)\n",
        "#   - Creating new tracks for unmatched detections\n",
        "#   - Removing stale tracks that haven't been seen recently\n",
        "\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "class Sort:\n",
        "    \"\"\"SORT: Simple Online and Realtime Tracking.\n",
        "\n",
        "    Args:\n",
        "        max_age: frames to keep a track alive without detections\n",
        "        min_hits: minimum detections before a track is reported\n",
        "        iou_threshold: minimum IoU for a valid detection-track match\n",
        "    \"\"\"\n",
        "    def __init__(self, max_age=3, min_hits=3, iou_threshold=0.3):\n",
        "        self.max_age = max_age\n",
        "        self.min_hits = min_hits\n",
        "        self.iou_threshold = iou_threshold\n",
        "        self.trackers = []\n",
        "        self.frame_count = 0\n",
        "\n",
        "    def update(self, detections):\n",
        "        \"\"\"Process detections for one frame and return tracked objects.\n",
        "\n",
        "        Args:\n",
        "            detections: Nx5 array of [x1,y1,x2,y2,score]\n",
        "\n",
        "        Returns:\n",
        "            Nx5 array of [x1,y1,x2,y2,track_id] for active tracks\n",
        "        \"\"\"\n",
        "        self.frame_count += 1\n",
        "\n",
        "        # Step 1: Predict new locations of all existing tracks\n",
        "        predicted = []\n",
        "        to_delete = []\n",
        "        for i, trk in enumerate(self.trackers):\n",
        "            pos = trk.predict()\n",
        "            if np.any(np.isnan(pos)):\n",
        "                to_delete.append(i)\n",
        "            else:\n",
        "                predicted.append(pos)\n",
        "        for i in reversed(to_delete):\n",
        "            self.trackers.pop(i)\n",
        "        predicted = np.array(predicted) if predicted else np.empty((0, 4))\n",
        "\n",
        "        # Step 2: Match detections to predictions using IoU + Hungarian\n",
        "        matched, unmatched_dets, unmatched_trks = self._associate(\n",
        "            detections, predicted\n",
        "        )\n",
        "\n",
        "        # Step 3: Update matched tracks with their new detections\n",
        "        for det_idx, trk_idx in matched:\n",
        "            self.trackers[trk_idx].update(detections[det_idx, :4])\n",
        "\n",
        "        # Step 4: Create new tracks for unmatched detections\n",
        "        for i in unmatched_dets:\n",
        "            self.trackers.append(KalmanBoxTracker(detections[i, :4]))\n",
        "\n",
        "        # Step 5: Remove dead tracks (not seen for max_age frames)\n",
        "        results = []\n",
        "        for trk in reversed(self.trackers):\n",
        "            d = trk.get_state()\n",
        "            # Only report tracks with enough consecutive hits\n",
        "            if trk.hit_streak >= self.min_hits or self.frame_count <= self.min_hits:\n",
        "                results.append(np.concatenate((d, [trk.id + 1])))\n",
        "            if trk.time_since_update > self.max_age:\n",
        "                self.trackers.remove(trk)\n",
        "\n",
        "        return np.array(results) if results else np.empty((0, 5))\n",
        "\n",
        "    def _associate(self, detections, predictions):\n",
        "        \"\"\"Match detections to predictions using IoU and the Hungarian algorithm.\"\"\"\n",
        "        if len(predictions) == 0:\n",
        "            return [], list(range(len(detections))), []\n",
        "        if len(detections) == 0:\n",
        "            return [], [], list(range(len(predictions)))\n",
        "\n",
        "        iou_matrix = iou_batch(detections[:, :4], predictions)\n",
        "        # Hungarian algorithm finds the optimal assignment\n",
        "        # We negate IoU because linear_sum_assignment minimizes cost\n",
        "        row_idx, col_idx = linear_sum_assignment(-iou_matrix)\n",
        "\n",
        "        matched, unmatched_dets, unmatched_trks = [], [], []\n",
        "        for d in range(len(detections)):\n",
        "            if d not in row_idx:\n",
        "                unmatched_dets.append(d)\n",
        "        for t in range(len(predictions)):\n",
        "            if t not in col_idx:\n",
        "                unmatched_trks.append(t)\n",
        "        for r, c in zip(row_idx, col_idx):\n",
        "            if iou_matrix[r, c] < self.iou_threshold:\n",
        "                unmatched_dets.append(r)\n",
        "                unmatched_trks.append(c)\n",
        "            else:\n",
        "                matched.append((r, c))\n",
        "        return matched, unmatched_dets, unmatched_trks\n",
        "\n",
        "print(\"SORT tracker defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxffOioCAj90"
      },
      "source": [
        "## 7. Evaluation — mAP, Tracking Accuracy & ID Switches\n",
        "We evaluate our system on two axes:\n",
        "1. **Detection quality** via mean Average Precision (mAP) — measures how accurate bounding boxes are.\n",
        "2. **Tracking quality** via MOTA (Multiple Object Tracking Accuracy) and number of ID Switches — measures temporal consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FY5hh_UAl3m"
      },
      "outputs": [],
      "source": [
        "# ── Detection Evaluation: mAP ─────────────────────────────────\n",
        "# mAP measures detection quality across different IoU thresholds.\n",
        "# We compute AP at IoU=0.50 (standard PASCAL VOC metric).\n",
        "# Higher mAP = better detection accuracy.\n",
        "\n",
        "from torchvision.ops import box_iou\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_map(model, data_loader, device, iou_threshold=0.5):\n",
        "    \"\"\"Compute mean Average Precision at a given IoU threshold.\n",
        "\n",
        "    We collect all predictions and ground-truths, then for each image:\n",
        "    1. Sort predictions by confidence score (descending)\n",
        "    2. Match each prediction to the best-overlapping ground-truth\n",
        "    3. Mark as TP if IoU >= threshold, else FP\n",
        "    4. Compute precision-recall curve and AP via trapezoidal integration\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_aps = []\n",
        "\n",
        "    for images, targets in data_loader:\n",
        "        images = [img.to(device) for img in images]\n",
        "        outputs = model(images)\n",
        "\n",
        "        for output, target in zip(outputs, targets):\n",
        "            pred_boxes = output['boxes'].cpu()\n",
        "            pred_scores = output['scores'].cpu()\n",
        "            gt_boxes = target['boxes'].cpu()\n",
        "\n",
        "            if len(gt_boxes) == 0 or len(pred_boxes) == 0:\n",
        "                if len(gt_boxes) == 0 and len(pred_boxes) == 0:\n",
        "                    all_aps.append(1.0)\n",
        "                else:\n",
        "                    all_aps.append(0.0)\n",
        "                continue\n",
        "\n",
        "            # Sort predictions by score (most confident first)\n",
        "            sorted_idx = torch.argsort(pred_scores, descending=True)\n",
        "            pred_boxes = pred_boxes[sorted_idx]\n",
        "            pred_scores = pred_scores[sorted_idx]\n",
        "\n",
        "            ious = box_iou(pred_boxes, gt_boxes)\n",
        "            gt_matched = torch.zeros(len(gt_boxes), dtype=torch.bool)\n",
        "\n",
        "            tp = torch.zeros(len(pred_boxes))\n",
        "            fp = torch.zeros(len(pred_boxes))\n",
        "\n",
        "            for i in range(len(pred_boxes)):\n",
        "                if len(gt_boxes) > 0:\n",
        "                    max_iou, max_idx = ious[i].max(0)\n",
        "                    if max_iou >= iou_threshold and not gt_matched[max_idx]:\n",
        "                        tp[i] = 1\n",
        "                        gt_matched[max_idx] = True\n",
        "                    else:\n",
        "                        fp[i] = 1\n",
        "                else:\n",
        "                    fp[i] = 1\n",
        "\n",
        "            tp_cumsum = torch.cumsum(tp, dim=0)\n",
        "            fp_cumsum = torch.cumsum(fp, dim=0)\n",
        "            precisions = tp_cumsum / (tp_cumsum + fp_cumsum)\n",
        "            recalls = tp_cumsum / len(gt_boxes)\n",
        "\n",
        "            # AP via trapezoidal rule on the precision-recall curve\n",
        "            ap = torch.trapezoid(precisions, recalls).item()\n",
        "            all_aps.append(max(0, ap))\n",
        "\n",
        "    return np.mean(all_aps) if all_aps else 0.0\n",
        "\n",
        "map_score = compute_map(model, val_loader, DEVICE)\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"  Detection mAP@0.50: {map_score:.4f}\")\n",
        "print(f\"{'='*50}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlDO-zSHAntR"
      },
      "outputs": [],
      "source": [
        "# ── Tracking Evaluation: MOTA, IDF1, ID Switches ─────────────\n",
        "# We run the full detect-then-track pipeline on a validation sequence\n",
        "# and compare tracked outputs against ground-truth using MOTMetrics.\n",
        "# - MOTA: overall tracking accuracy (higher = better)\n",
        "# - IDF1: identification F1 score (higher = better identity preservation)\n",
        "# - ID Switches: number of times a tracked ID changes (lower = better)\n",
        "\n",
        "# --- BEGIN PATCH FOR NUMPY 2.0 COMPATIBILITY ---\n",
        "def _patched_iou_matrix(objs, hyps, max_iou=None):\n",
        "    if objs.shape[0] == 0 and hyps.shape[0] == 0:\n",
        "        return np.empty((0, 0))\n",
        "    if objs.shape[0] == 0:\n",
        "        return np.empty((0, hyps.shape[0]))\n",
        "    if hyps.shape[0] == 0:\n",
        "        return np.empty((objs.shape[0], 0))\n",
        "\n",
        "    # Fix: replace np.asfarray with np.asarray(..., dtype=np.float64)\n",
        "    objs = np.asarray(objs, dtype=np.float64)\n",
        "    hyps = np.asarray(hyps, dtype=np.float64)\n",
        "    assert objs.shape[1] == 4\n",
        "    assert hyps.shape[1] == 4\n",
        "\n",
        "    # Extract coordinates for intersection\n",
        "    tl_br_o = objs[:, [0, 1, 2, 3]]\n",
        "    tl_br_h = hyps[:, [0, 1, 2, 3]]\n",
        "\n",
        "    # Calculate intersection\n",
        "    tl_i = np.maximum(tl_br_o[:, None, :2], tl_br_h[:, :2])\n",
        "    br_i = np.minimum(tl_br_o[:, None, 2:], tl_br_h[:, 2:])\n",
        "    wh_i = np.maximum(0., br_i - tl_i)\n",
        "    area_i = np.prod(wh_i, axis=2)\n",
        "\n",
        "    # Calculate areas of objects and hypotheses\n",
        "    area_o = np.prod(tl_br_o[:, 2:] - tl_br_o[:, :2], axis=1)\n",
        "    area_h = np.prod(tl_br_h[:, 2:] - tl_br_h[:, :2], axis=1)\n",
        "\n",
        "    # Calculate IoU\n",
        "    area_u = area_o[:, None] + area_h - area_i\n",
        "    iou = area_i / np.maximum(1., area_u)\n",
        "\n",
        "    if max_iou is None:\n",
        "        return 1 - iou\n",
        "    else:\n",
        "        return np.maximum(0, 1 - iou / max_iou)\n",
        "\n",
        "mm.distances.iou_matrix = _patched_iou_matrix\n",
        "# --- END PATCH FOR NUMPY 2.0 COMPATIBILITY ---\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_tracking_evaluation(model, samples, device, score_thresh=0.5):\n",
        "    \"\"\"Run detection + SORT tracking on a sequence and compute MOT metrics.\"\"\"\n",
        "    model.eval()\n",
        "    tracker = Sort(max_age=5, min_hits=2, iou_threshold=0.3)\n",
        "    KalmanBoxTracker.count = 0  # Reset ID counter\n",
        "\n",
        "    accumulator = mm.MOTAccumulator(auto_id=True)\n",
        "\n",
        "    for idx, (img_path, annots) in enumerate(samples):\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        img_tensor = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
        "        img_tensor = img_tensor.to(device)\n",
        "\n",
        "        # Run detection\n",
        "        output = model([img_tensor])[0]\n",
        "        keep = output['scores'] > score_thresh\n",
        "        det_boxes = output['boxes'][keep].cpu().numpy()\n",
        "        det_scores = output['scores'][keep].cpu().numpy()\n",
        "\n",
        "        # Format for SORT: [x1,y1,x2,y2,score]\n",
        "        if len(det_boxes) > 0:\n",
        "            dets = np.column_stack([det_boxes, det_scores])\n",
        "        else:\n",
        "            dets = np.empty((0, 5))\n",
        "\n",
        "        # Run SORT tracker\n",
        "        tracked = tracker.update(dets)\n",
        "\n",
        "        # Ground truth for this frame\n",
        "        gt_ids = [a['track_id'] for a in annots]\n",
        "        gt_boxes = np.array([a['bbox'] for a in annots]) if annots else np.empty((0, 4))\n",
        "\n",
        "        # Tracked results\n",
        "        trk_ids = tracked[:, 4].astype(int).tolist() if len(tracked) > 0 else []\n",
        "        trk_boxes = tracked[:, :4] if len(tracked) > 0 else np.empty((0, 4))\n",
        "\n",
        "        # Compute distance matrix (1 - IoU) for MOTMetrics\n",
        "        if len(gt_boxes) > 0 and len(trk_boxes) > 0:\n",
        "            dist_matrix = mm.distances.iou_matrix(gt_boxes, trk_boxes, max_iou=0.5)\n",
        "        else:\n",
        "            dist_matrix = np.empty((len(gt_boxes), len(trk_boxes))) # This line remains as is\n",
        "\n",
        "        accumulator.update(gt_ids, trk_ids, dist_matrix)\n",
        "\n",
        "    # Compute summary metrics\n",
        "    mh = mm.metrics.create()\n",
        "    summary = mh.compute(accumulator, metrics=[\n",
        "        'mota', 'idf1', 'num_switches',\n",
        "        'mostly_tracked', 'mostly_lost',\n",
        "        'num_fragmentations', 'precision', 'recall'\n",
        "    ], name='MOT17-eval')\n",
        "\n",
        "    return summary\n",
        "\n",
        "# Evaluate on validation samples (use first 200 for speed)\n",
        "eval_samples = val_samples[:200]\n",
        "tracking_summary = run_tracking_evaluation(model, eval_samples, DEVICE)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"  TRACKING EVALUATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(tracking_summary.to_string())\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb4fe4b5"
      },
      "source": [
        "### Experiment: Lowering Detection Confidence Threshold\n",
        "\n",
        "Let's re-run the tracking evaluation with a lower `score_thresh` to see how it impacts recall and other tracking metrics. We'll try `0.3` instead of `0.5`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71531131"
      },
      "outputs": [],
      "source": [
        "# Evaluate on validation samples with a lower score threshold\n",
        "# Using the same eval_samples as before for consistent comparison\n",
        "low_thresh_tracking_summary = run_tracking_evaluation(model, eval_samples, DEVICE, score_thresh=0.3)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"  TRACKING EVALUATION RESULTS (Score Threshold = 0.3)\")\n",
        "print(\"=\"*60)\n",
        "print(low_thresh_tracking_summary.to_string())\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Compare with previous results\n",
        "print(\"\\nPrevious MOTA (thresh=0.5):\", tracking_summary['mota'].values[0])\n",
        "print(\"New MOTA (thresh=0.3):\", low_thresh_tracking_summary['mota'].values[0])\n",
        "\n",
        "print(\"Previous Recall (thresh=0.5):\", tracking_summary['recall'].values[0])\n",
        "print(\"New Recall (thresh=0.3):\", low_thresh_tracking_summary['recall'].values[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-1olllaApn7"
      },
      "outputs": [],
      "source": [
        "# ── Summary of All Metrics ────────────────────────────────────\n",
        "# Print a clean, consolidated view of all evaluation metrics.\n",
        "\n",
        "mota = tracking_summary['mota'].values[0]\n",
        "idf1 = tracking_summary['idf1'].values[0]\n",
        "id_switches = tracking_summary['num_switches'].values[0]\n",
        "\n",
        "print(\"\\n\" + \"╔\" + \"═\"*56 + \"╗\")\n",
        "print(\"║{:^56s}║\".format(\"FINAL EVALUATION METRICS\"))\n",
        "print(\"╠\" + \"═\"*56 + \"╣\")\n",
        "print(\"║  {:<35s} {:>15.4f}  ║\".format(\"Detection mAP@0.50:\", map_score))\n",
        "print(\"║  {:<35s} {:>15.4f}  ║\".format(\"Tracking Accuracy (MOTA):\", mota))\n",
        "print(\"║  {:<35s} {:>15.4f}  ║\".format(\"ID F1 Score (IDF1):\", idf1))\n",
        "print(\"║  {:<35s} {:>15d}  ║\".format(\"Identity Switches:\", int(id_switches)))\n",
        "print(\"╚\" + \"═\"*56 + \"╝\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tFXluLPAs1q"
      },
      "source": [
        "## 8. Model Analysis & Justification\n",
        "\n",
        "### Why Faster R-CNN?\n",
        "\n",
        "**Faster R-CNN** was selected as the detection backbone for several compelling reasons:\n",
        "\n",
        "1. **Two-Stage Accuracy**: Unlike single-shot detectors (YOLO, SSD), Faster R-CNN uses a Region Proposal Network (RPN) followed by a refinement stage. This two-stage approach yields significantly higher detection accuracy, especially for smaller and partially occluded pedestrians — a common challenge in surveillance/MOT scenarios.\n",
        "\n",
        "2. **Feature Pyramid Network (FPN)**: The ResNet-50 + FPN backbone provides multi-scale feature maps, enabling robust detection of objects at various sizes within the same frame. This is crucial for MOT17 where camera distances vary significantly across sequences.\n",
        "\n",
        "3. **Transfer Learning from COCO**: The pre-trained weights from COCO (which includes the \"person\" class) provide an excellent initialization. Fine-tuning only the classification head allows us to achieve strong performance with minimal training data and compute time.\n",
        "\n",
        "4. **Rich Box Regression**: The second-stage box refinement in Faster R-CNN produces more precise bounding boxes compared to single-shot methods, which directly improves tracking IoU matching quality.\n",
        "\n",
        "### How SORT Improves Temporal Consistency\n",
        "\n",
        "The **SORT (Simple Online and Realtime Tracking)** algorithm provides several key benefits:\n",
        "\n",
        "1. **Kalman Filter Prediction**: By modeling each track with a constant-velocity Kalman Filter, SORT can predict where objects will appear in the next frame. This bridges brief detection gaps caused by occlusion, motion blur, or missed detections.\n",
        "\n",
        "2. **Hungarian Matching**: The optimal assignment between predictions and new detections (via the Hungarian algorithm) ensures that object identities are preserved even when objects cross paths or temporarily overlap.\n",
        "\n",
        "3. **Track Lifecycle Management**: SORT's `max_age` and `min_hits` parameters control when tracks are created and terminated. This prevents spurious false-positive tracks (by requiring `min_hits` consecutive detections) and tolerates brief occlusions (by keeping tracks alive for `max_age` frames).\n",
        "\n",
        "4. **Low Computational Overhead**: SORT adds negligible computation beyond the detection model, making the entire pipeline feasible for real-time applications.\n",
        "\n",
        "### Overfitting / Underfitting Analysis\n",
        "\n",
        "- **Pre-training mitigates overfitting**: Since we use COCO-pretrained weights, the feature extractor already encodes general visual patterns. Fine-tuning only the head layers with a small learning rate (0.005 → 0.0005 via StepLR) prevents overfitting to the limited MOT17 training data.\n",
        "\n",
        "- **Data augmentation as regularization**: Random cropping, horizontal flipping, and color jittering serve as implicit regularization, effectively expanding the training distribution and reducing the model's tendency to memorize specific frames.\n",
        "\n",
        "- **Monitoring**: The training loss curve provides a diagnostic — a smoothly decreasing curve without sudden spikes indicates healthy convergence. If the loss plateaued at a high value (underfitting), we would increase model capacity or training duration.\n",
        "\n",
        "### Speed vs. Accuracy Trade-off\n",
        "\n",
        "| Approach | Speed (FPS) | mAP | Use Case |\n",
        "|----------|:-----------:|:---:|----------|\n",
        "| YOLOv5 (single-shot) | ~60 | ~0.65 | Real-time surveillance |\n",
        "| Faster R-CNN (ours) | ~10-15 | ~0.80+ | Accuracy-critical analysis |\n",
        "| Cascade R-CNN | ~5-8 | ~0.85 | Offline forensic analysis |\n",
        "\n",
        "Our choice of Faster R-CNN prioritizes **detection accuracy over raw speed**. For the MOT17 benchmark (which is evaluated offline), this is the right trade-off. In a deployment scenario requiring real-time performance (>30 FPS), one could:\n",
        "- Switch to YOLOv5/v8 as the detector while keeping the SORT tracker\n",
        "- Use TensorRT or ONNX optimization to accelerate inference\n",
        "- Reduce input resolution (with an expected drop in detection quality)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJcDvBGh48gW"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eQqGJaNA2H7"
      },
      "source": [
        "## 9. Conclusion\n",
        "\n",
        "In this notebook, we built a complete **object detection and tracking pipeline** for multi-object tracking in video streams:\n",
        "\n",
        "1. **Data Preprocessing**: Downloaded and parsed the MOT17 dataset, applied normalization and three augmentation techniques (random cropping, horizontal flipping, color jittering) with proper bounding box coordination via `albumentations`.\n",
        "\n",
        "2. **Model**: Fine-tuned a **Faster R-CNN (ResNet-50 FPN v2)** pre-trained on COCO, adapting it for single-class pedestrian detection. The two-stage architecture provides high-quality detections essential for downstream tracking.\n",
        "\n",
        "3. **Tracking**: Implemented **SORT** with Kalman Filter–based motion prediction and Hungarian algorithm matching. This maintains consistent object identities across frames, handling brief occlusions and detection gaps.\n",
        "\n",
        "4. **Evaluation**: Measured **mAP@0.50** for detection quality, and **MOTA**, **IDF1**, and **ID Switches** for tracking performance using the standard `motmetrics` library.\n",
        "\n",
        "The system demonstrates that combining a strong detection backbone with a lightweight tracking algorithm can achieve robust multi-object tracking suitable for surveillance, autonomous driving, and sports analytics applications.\n",
        "\n",
        "---\n",
        "*Notebook by Group 23 — Assignment 2, Computer Vision*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}